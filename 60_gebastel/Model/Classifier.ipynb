{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac081af5-fa27-4465-a577-742a38bf36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../30_data_tools/')\n",
    "from pathlib import Path\n",
    "from random import choices\n",
    "from PIL import Image, ImageChops, ImageEnhance, ImageDraw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import sqlite3\n",
    "import re\n",
    "import cv2\n",
    "# import YOLO model\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4901dc7a-7e91-471e-8477-d9d15555d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import load_dotenv\n",
    "from get_labelstudio_data import get_results_of_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439b3c69-ae47-4a68-ac19-71f05d74d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 280\n",
    "MIN_MIDTONE_SHARE = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a4919db-e677-48b0-9c71-47aae0c52a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be2e21a-3045-49aa-9848-8e2b7df89e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect( dotenv['DB_PATH'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04623b9a-186c-42d0-b98b-d46d1f51e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_jobs = [\n",
    "    '23-10-03_Testformen',\n",
    "    '23-10-18_farbe',\n",
    "    '23-10-17_blur',\n",
    "    '23-10-19_farbe'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "906e9b21-d867-45a6-a045-49ab20dfa8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    r for r in get_results_of_project(1)\n",
    "    if r['rectanglelabels'][0] not in ['potential_moire'] and True not in [r['img_name'].startswith(ej) for ej in exclude_jobs]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3272ea9-8c0c-452f-8ad1-d319120b31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pd.read_sql(\n",
    "    '''\n",
    "        SELECT * FROM related_file rf \n",
    "        WHERE variant_name = 'ps2400dpi150lpi' AND \"type\" = '4c_600'\n",
    "    ''',\n",
    "    con\n",
    ")\n",
    "pages.loc[\n",
    "    :,\n",
    "    'dpi'\n",
    "] = pd.to_numeric(pages.filename.str.extract(r'.+\\.4c_(\\d+)\\.jpg')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a5a11bd-c7f2-4b68-b82f-244a25d338c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(dotenv['MODEL_DIR'] / 'yolov8_moires_24-02-22.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34b7554a-6aa8-4323-86ff-4678973b1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_1_classifier( img ):\n",
    "    relevant_tiles = []    \n",
    "    left = 0\n",
    "    top = 0\n",
    "    \n",
    "    while left < img.size[0] or top < img.size[1]:\n",
    "        tile = img.crop((\n",
    "            left,top,\n",
    "            left+TILE_SIZE,top+TILE_SIZE\n",
    "        ))\n",
    "        np_tile = np.array(tile)\n",
    "        is_relevant = False\n",
    "        \n",
    "        for i in range(np_tile.shape[2]):\n",
    "            sep = np_tile[:,:,i]\n",
    "        \n",
    "            midtone_share = sep[(sep > 8) & (sep < 247)].shape[0] / (sep.shape[0] * sep.shape[1])\n",
    "            if midtone_share > MIN_MIDTONE_SHARE:\n",
    "                is_relevant = True\n",
    "                break\n",
    "    \n",
    "        if is_relevant:\n",
    "            relevant_tiles.append(((left, top),tile))\n",
    "    \n",
    "        if left < img.size[0]:\n",
    "            left += round(TILE_SIZE / 2)\n",
    "        elif top < img.size[1]:\n",
    "            left = 0\n",
    "            top += round(TILE_SIZE / 2)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return relevant_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96f31b3f-3372-43e5-a343-23a6316a2278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_2_classifier( tiles ):\n",
    "    classifications = []\n",
    "\n",
    "    for pos,tile in tiles:\n",
    "        tile_to_process = Image.fromarray(np.array(tile)[:,:,3]).convert('RGB')\n",
    "        \n",
    "        pred = model.predict(tile_to_process, verbose=False)[0].probs.data\n",
    "        target_label = 1 - int(torch.argmax(pred))\n",
    "        classifications.append((\n",
    "            pos,\n",
    "            tile,\n",
    "            target_label,\n",
    "            pred\n",
    "        ))\n",
    "    \n",
    "    return classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bc782bc-7e63-4a04-83b7-9bb1d9c1afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_out_map( img, relevant_tiles ):\n",
    "    out_img = np.zeros((img.size[1], img.size[0], 4))\n",
    "    colors = [\n",
    "        (0,0,255),\n",
    "        (255,0,0)\n",
    "    ]\n",
    "    \n",
    "    for pos,tile,label,pred in relevant_tiles:\n",
    "        out_img[\n",
    "            pos[1]:pos[1]+TILE_SIZE,\n",
    "            pos[0]:pos[0]+TILE_SIZE,\n",
    "            0\n",
    "        ] += 1\n",
    "\n",
    "        out_img[\n",
    "            pos[1]:pos[1]+TILE_SIZE,\n",
    "            pos[0]:pos[0]+TILE_SIZE,\n",
    "            1\n",
    "        ] += float(pred[1])\n",
    "\n",
    "        out_img[\n",
    "            pos[1]:pos[1]+TILE_SIZE,\n",
    "            pos[0]:pos[0]+TILE_SIZE,\n",
    "            2\n",
    "        ] += float(pred[0])\n",
    "\n",
    "    out_img /= 4\n",
    "\n",
    "    level_1_img = Image.fromarray((out_img[:,:,0] * 255).astype('uint8'))\n",
    "    level_2_img = Image.fromarray((out_img[:,:,1] * 255).astype('uint8'))\n",
    "    level_3_img = Image.fromarray((out_img[:,:,2] * 255).astype('uint8'))\n",
    "    level_4_img = out_img[:,:,2] * 255\n",
    "    level_4_img[(out_img[:,:,2] < out_img[:,:,1]) | (out_img[:,:,2] < 0.5)] = 0\n",
    "    level_4_img = Image.fromarray(level_4_img.astype('uint8'))\n",
    "    \n",
    "    return level_1_img, level_2_img, level_3_img, level_4_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0f36adc-51e8-4639-8fdf-b841f544c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result_map( img_size, tiles ):\n",
    "    result_map = np.zeros((img_size[1],img_size[0])).astype('float32')\n",
    "    \n",
    "    for pos,tile,label,pred in tiles:\n",
    "        if pred[1] < pred[0]:\n",
    "            result_map[\n",
    "                pos[1]:pos[1]+TILE_SIZE,\n",
    "                pos[0]:pos[0]+TILE_SIZE,\n",
    "            ] += float(pred[0]) / 4\n",
    "            # / 4 weil max. vier tiles die Kachel bestimmen\n",
    "\n",
    "    return result_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e94f8a5f-e728-4614-bba7-86d2c98bd60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moires_of_page( row, results ):\n",
    "    relevant_moires = []\n",
    "    \n",
    "    for r in results:\n",
    "        if re.match(f'^{ row.job }\\..+?\\.{ row.pdf_filename }\\..+', r['img_name']):\n",
    "            dpi = int( re.match(r'.+\\.4c_(\\d+)\\.jpg', r['img_name']).groups()[0] )\n",
    "            out_box = [r['value']['x'],r['value']['y'],r['value']['width'],r['value']['height']]\n",
    "            \n",
    "            # box umrechnen\n",
    "            if target_dpi != dpi:\n",
    "                out_box = [round(val * (target_dpi / dpi)) for val in out_box]\n",
    "    \n",
    "            relevant_moires.append(out_box)\n",
    "\n",
    "    return relevant_moires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4734e48-daa2-46b9-a96f-68ff36ae28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection_over_union( box_a, box_b ):\n",
    "    intersection_box = [\n",
    "        max(box_a[0],box_b[0]),\n",
    "        max(box_a[1],box_b[1]),\n",
    "        min(box_a[0]+box_a[2],box_b[0]+box_b[2]),\n",
    "        min(box_a[1]+box_a[3],box_b[1]+box_b[3]),\n",
    "    ]\n",
    "\n",
    "    if intersection_box[2] - intersection_box[0] < 0 or intersection_box[3] - intersection_box[1] < 0:\n",
    "        return 0\n",
    "\n",
    "    intersection = (intersection_box[2] - intersection_box[0]) * (intersection_box[3] - intersection_box[1])\n",
    "    \n",
    "    union_box = [\n",
    "        min(box_a[0],box_b[0]),\n",
    "        min(box_a[1],box_b[1]),\n",
    "        max(box_a[0]+box_a[2],box_b[0]+box_b[2]),\n",
    "        max(box_a[1]+box_a[3],box_b[1]+box_b[3]),\n",
    "    ]\n",
    "    union = (union_box[2] - union_box[0]) * (union_box[3] - union_box[1])\n",
    "\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ae0d8b1-32cc-4f80-bd37-38f59b06840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_boxes( result_map, threshold=0.5 ):\n",
    "    thresh = np.zeros(result_map.shape).astype('uint8')\n",
    "    thresh[result_map > threshold] = 255\n",
    "\n",
    "    (numLabels, labels, stats, centroids) = cv2.connectedComponentsWithStats(\n",
    "    \tthresh, 4, cv2.CV_32S\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        [b[0],b[1],b[2],b[3]]\n",
    "        for b in stats[1:]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e0698b7-2217-43b8-83c5-6d9b219cbb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_boxes( img, moire_boxes, predicted_boxes ):\n",
    "    colors = {\n",
    "        \"target\" : \"green\",\n",
    "        \"predicted\" : \"red\"\n",
    "    }    \n",
    "\n",
    "    img_out = img.copy().convert('RGB')\n",
    "    draw = ImageDraw.Draw(img_out) \n",
    "\n",
    "    for b in moire_boxes:\n",
    "        draw.rectangle([b[0],b[1],b[0]+b[2],b[1]+b[3]], outline=colors['target'], width=10) \n",
    "\n",
    "    for b in predicted_boxes:\n",
    "        draw.rectangle([b[0],b[1],b[0]+b[2],b[1]+b[3]], outline=colors['predicted'], width=10) \n",
    "\n",
    "    \n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e93a4fa-3c99-46d5-a86a-0d0b0479485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dpi = dotenv['TRAIN_DATA_DPI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29ecad77-ab1e-4da8-8b15-7a3aeece4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_idx = []\n",
    "\n",
    "for r in results:\n",
    "    res = re.match(f'^(.+?)\\..+?\\.(.+?)\\.4c_\\d+\\.jpg', r['img_name'])\n",
    "    try:\n",
    "        job, pdf_filename = res.groups()\n",
    "    \n",
    "        relevant_idx.append(\n",
    "            pages.loc[\n",
    "                (pages.job == job) &\n",
    "                (pages.pdf_filename == pdf_filename)\n",
    "            ].iloc[0].name\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "relevant_idx = list(set(relevant_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e73f35e-3817-44e6-a052-97adef0290af",
   "metadata": {},
   "outputs": [],
   "source": [
    "0 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d2c833a-c147-4cf8-a810-be4b5c5482b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "68b64bdc-951b-4a69-a498-65faf13ef8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.concat([\n",
    "    pages.loc[\n",
    "        pages.index.isin([o[0] for o in out]) == False\n",
    "    ].sample(n=40),\n",
    "    pages.loc[\n",
    "        (pages.index.isin([o[0] for o in out]) == False) &\n",
    "        (pages.index.isin(choices(relevant_idx, k=20)))\n",
    "    ]\n",
    "]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2c532d4b-a132-4f2d-b624-cbb97eea8250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f680908ef849f2a242de08808167d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(samples.shape[0])):\n",
    "    sample = samples.iloc[i]\n",
    "    \n",
    "    img_path = dotenv['DATA_DIR'] / sample.job / sample.variant_name / sample.filename\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize((\n",
    "        round(img.size[0] * (target_dpi / sample.dpi)),\n",
    "        round(img.size[1] * (target_dpi / sample.dpi))\n",
    "    ))\n",
    "    relevant_tiles = step_1_classifier( img )\n",
    "    classifications = step_2_classifier( relevant_tiles )\n",
    "\n",
    "    result_map = create_result_map( img.size, classifications )\n",
    "    relevant_moires = get_moires_of_page( sample, results )\n",
    "\n",
    "    predicted_label = int(result_map[result_map > 0.5].shape[0] > 0)\n",
    "    label = int(len(relevant_moires) > 0)\n",
    "\n",
    "    if label == 1 or predicted_label == 1:\n",
    "        result_boxes = get_result_boxes( result_map ) if predicted_label == 1 else []\n",
    "\n",
    "        if len(result_boxes) > 0:\n",
    "            out.append((\n",
    "                sample.name, predicted_label, label, [max([get_intersection_over_union(r,p) for p in result_boxes]) for r in relevant_moires]\n",
    "            ))\n",
    "        else:\n",
    "            out.append((\n",
    "                sample.name, predicted_label, label, []\n",
    "            ))\n",
    "    \n",
    "        draw_bounding_boxes( img, relevant_moires, result_boxes ).save(f'./temp/{ sample.filename }')\n",
    "    else:\n",
    "        out.append((\n",
    "            sample.name, predicted_label, label, []\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "98582be5-bed5-469e-a1af-890b13afb6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages.loc[\n",
    "    [o[0] for o in out],\n",
    "    'predicted_label'\n",
    "] = [o[1] for o in out]\n",
    "\n",
    "pages.loc[\n",
    "    [o[0] for o in out],\n",
    "    'label'\n",
    "] = [o[2] for o in out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "08fc28fb-f96b-47dc-9e73-f5c99779464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages.loc[\n",
    "    pd.isna(pages.predicted_label) == False\n",
    "].to_pickle('./temp/out.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6d097c1b-bb81-4b6e-842b-a26e776f34a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = pages.loc[\n",
    "    (pages.label == 1) &\n",
    "    (pages.predicted_label == 1)\n",
    "].shape[0]\n",
    "TN = pages.loc[\n",
    "    (pages.label == 0) &\n",
    "    (pages.predicted_label == 0)\n",
    "].shape[0]\n",
    "FP = pages.loc[\n",
    "    (pages.label == 0) &\n",
    "    (pages.predicted_label == 1)\n",
    "].shape[0]\n",
    "FN = pages.loc[\n",
    "    (pages.label == 1) &\n",
    "    (pages.predicted_label == 0)\n",
    "].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "39e5a38f-6062-4ff2-bb7b-a72448bcaab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = TP / (TP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "correct_values = TP + TN\n",
    "accuracy = correct_values / len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f4aeef5c-15e6-4da9-b3cc-758a03f31112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 140, 57, 0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "be580f5e-3d18-49f5-ab3d-ea3244483968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7446808510638298, 1.0, 0.3804347826086957)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c692d6a-bfb1-443a-ba5c-07a7dec12fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "0 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ef943cba-7e22-43a1-8d2c-20ddd09eb8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pages.loc[\n",
    "    (pages.predicted_label == 1) & (pages.label == 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4639bcf9-88e3-4703-a1ba-4ce44fa278a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab09b89447f4037bd26f814956aff90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(samples.shape[0])):\n",
    "    sample = samples.iloc[i]\n",
    "    resolution = sample.dpi\n",
    "    img_path = dotenv['DATA_DIR'] / sample.job / sample.variant_name / sample.filename\n",
    "\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize((\n",
    "        round(img.size[0] * (target_dpi / resolution)),\n",
    "        round(img.size[1] * (target_dpi / resolution))\n",
    "    ))\n",
    "    relevant_tiles = step_1_classifier( img )\n",
    "    classifications = step_2_classifier( relevant_tiles )\n",
    "\n",
    "    # blend img erzeugen\n",
    "    TARGET_OUT_HEIGHT = 1000\n",
    "    colors=['red','green','blue','orange']\n",
    "    l_images = get_out_map( img, classifications )\n",
    "    blended = Image.new(mode=\"RGB\", size=(img.size[0] * len(l_images), img.size[1]))\n",
    "\n",
    "    for i in range(len(l_images)):\n",
    "        l_img = ImageEnhance.Brightness(l_images[i]).enhance(0.5)\n",
    "        overlay = Image.new('RGB', l_img.size, color=colors[i])\n",
    "        l_rgb = img.convert('RGB')\n",
    "        l_rgb.paste(\n",
    "            overlay,\n",
    "            (0,0),\n",
    "            mask=l_img\n",
    "        )\n",
    "\n",
    "        blended.paste(\n",
    "            l_rgb,\n",
    "            (img.size[0] * i,0)\n",
    "        )\n",
    "\n",
    "    blended = blended.resize((\n",
    "        round(TARGET_OUT_HEIGHT / blended.size[1] * blended.size[0]),\n",
    "        TARGET_OUT_HEIGHT\n",
    "    ))\n",
    "    \n",
    "    blended.save( Path('./result_images') / f\"separated_{sample.name}.jpg\", progressive=True )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
