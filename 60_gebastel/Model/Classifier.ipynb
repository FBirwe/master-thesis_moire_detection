{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac081af5-fa27-4465-a577-742a38bf36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../30_data_tools/')\n",
    "from pathlib import Path\n",
    "from random import choices\n",
    "from PIL import Image, ImageChops, ImageEnhance, ImageDraw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from file_interaction import download_blob, get_related_filepath, open_img\n",
    "from tqdm.auto import tqdm\n",
    "import sqlite3\n",
    "import re\n",
    "import cv2\n",
    "# import YOLO model\n",
    "from ultralytics import YOLO\n",
    "from io import BytesIO\n",
    "from torchvision.transforms.functional import pil_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4901dc7a-7e91-471e-8477-d9d15555d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import load_dotenv\n",
    "from get_labelstudio_data import get_results_of_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3ca4c976-5080-4901-818c-cd690d7ab77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439b3c69-ae47-4a68-ac19-71f05d74d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 224\n",
    "MIN_MIDTONE_SHARE = 0.05\n",
    "MARGIN = 0.03\n",
    "MAX_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cfcbd5a-9e00-4cb6-a7b3-474884cbf557",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '2024-04-11_resnet50_003'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4919db-e677-48b0-9c71-47aae0c52a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv = load_dotenv()\n",
    "con = sqlite3.connect( dotenv['DB_PATH'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04623b9a-186c-42d0-b98b-d46d1f51e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_jobs = [\n",
    "    '23-10-03_Testformen',\n",
    "    '23-10-18_farbe',\n",
    "    '23-10-17_blur',\n",
    "    '23-10-19_farbe'\n",
    "]\n",
    "\n",
    "include_jobs = [\n",
    "    '24-03-05-01_randomTrainPages'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "906e9b21-d867-45a6-a045-49ab20dfa8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    r for r in get_results_of_project(3)\n",
    "    if r['labels'][0] not in ['potential_moire'] and True not in [r['img_name'].startswith(ej) for ej in exclude_jobs]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5a9cc51-35f2-4963-ab78-c674f1defcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pd.read_sql(\n",
    "    f'''\n",
    "        SELECT rf.* FROM related_file rf \n",
    "        LEFT JOIN (SELECT job, pdf_filename, 1 AS used_as_base FROM generic_image gi ) gi\n",
    "        ON rf.job=gi.job AND rf.pdf_filename = gi.pdf_filename\n",
    "        WHERE variant_name = 'ps2400dpi150lpi' AND \"type\" = '4c_{ dotenv[\"LOFI_DPI\"] }' AND gi.used_as_base IS NULL\n",
    "    ''',\n",
    "    con\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "316651fa-d0c4-4ad1-bd50-8233c13f2b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytesStream = download_blob( f'models/{ model_name }.pth' )\n",
    "model = torch.load( BytesIO(bytesStream.getvalue()), map_location=torch.device('cpu') )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d0538c-f278-473e-abbe-eeff319cde9b",
   "metadata": {},
   "source": [
    "# Funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "822fbf43-9952-4fb2-b7e3-62ac5cfed41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_img_into_tiles( img ):\n",
    "    tiles = []    \n",
    "    left = 0\n",
    "    top = 0\n",
    "    \n",
    "    while left < img.size[0] or top < img.size[1]:\n",
    "        tile = img.crop((\n",
    "            left,top,\n",
    "            left+TILE_SIZE,top+TILE_SIZE\n",
    "        ))\n",
    "        tiles.append( ((left,top),tile) )\n",
    "    \n",
    "        if left < img.size[0]:\n",
    "            left += round(TILE_SIZE / 2)\n",
    "        elif top < img.size[1]:\n",
    "            left = 0\n",
    "            top += TILE_SIZE\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "34b7554a-6aa8-4323-86ff-4678973b1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preclassifier( tiles ):\n",
    "    tiles_out = []\n",
    "    \n",
    "    for pos,tile in tiles:\n",
    "        is_relevant = False\n",
    "        np_tile = 1 - np.array(tile) / 255\n",
    "        \n",
    "        for i in range( np_tile.shape[2] ):\n",
    "            sep = np_tile[:,:,i]\n",
    "            midtone_share = sep[(sep > MARGIN) & (sep < (1 - MARGIN))].shape[0] / (sep.shape[0] * sep.shape[1])\n",
    "        \n",
    "            if midtone_share > MIN_MIDTONE_SHARE:\n",
    "                is_relevant = True\n",
    "                break\n",
    "\n",
    "        if is_relevant:\n",
    "            tiles_out.append((pos,tile))\n",
    "\n",
    "    return tiles_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "96f31b3f-3372-43e5-a343-23a6316a2278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier( tiles ):\n",
    "    batch_start = 0\n",
    "    predictions = []\n",
    "    \n",
    "    with tqdm(total=len(tiles)) as pbar:\n",
    "        while batch_start < len(tiles):\n",
    "            tile_selection = tiles[batch_start:batch_start+MAX_BATCH_SIZE]\n",
    "            batch = torch.zeros([len(tile_selection), 3, TILE_SIZE, TILE_SIZE], dtype=torch.float32)\n",
    "        \n",
    "            for i in range(len(tile_selection)):\n",
    "                batch[i,:,:,:] = 1 - pil_to_tensor(Image.fromarray(np.array(tile_selection[i][1])[:,:,3]).convert('RGB')) / 255\n",
    "            \n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model_predictions = model(batch)\n",
    "                predictions += [(*tile_selection[i],model_predictions[i],int(torch.argmax(model_predictions[i]))) for i in range(len(model_predictions))]\n",
    "    \n",
    "            batch_start += MAX_BATCH_SIZE\n",
    "            pbar.update(len(tile_selection))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4bc782bc-7e63-4a04-83b7-9bb1d9c1afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_out_map( img, relevant_tiles ):\n",
    "    out_img = np.zeros((img.size[1], img.size[0], 4))\n",
    "    colors = [\n",
    "        (0,0,255),\n",
    "        (255,0,0)\n",
    "    ]\n",
    "    \n",
    "    for pos,tile,pred,label in relevant_tiles:\n",
    "        out_img[\n",
    "            pos[1]:pos[1]+TILE_SIZE,\n",
    "            pos[0]:pos[0]+TILE_SIZE,\n",
    "            0\n",
    "        ] += 1\n",
    "\n",
    "        out_img[\n",
    "            pos[1]:pos[1]+TILE_SIZE,\n",
    "            pos[0]:pos[0]+TILE_SIZE,\n",
    "            1\n",
    "        ] += float(pred[1])\n",
    "\n",
    "        out_img[\n",
    "            pos[1]:pos[1]+TILE_SIZE,\n",
    "            pos[0]:pos[0]+TILE_SIZE,\n",
    "            2\n",
    "        ] += float(pred[0])\n",
    "\n",
    "    out_img /= 2\n",
    "\n",
    "    level_1_img = Image.fromarray((out_img[:,:,0] * 255).astype('uint8'))\n",
    "    level_2_img = Image.fromarray((out_img[:,:,1] * 255).astype('uint8'))\n",
    "    level_3_img = Image.fromarray((out_img[:,:,2] * 255).astype('uint8'))\n",
    "    level_4_img = out_img[:,:,2] * 255\n",
    "    level_4_img[(out_img[:,:,2] < out_img[:,:,1]) | (out_img[:,:,2] < 0.5)] = 0\n",
    "    level_4_img = Image.fromarray(level_4_img.astype('uint8'))\n",
    "    \n",
    "    return level_1_img, level_2_img, level_3_img, level_4_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0f36adc-51e8-4639-8fdf-b841f544c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result_map( img_size, tiles ):\n",
    "    result_map = np.zeros((img_size[1],img_size[0])).astype('float32')\n",
    "    \n",
    "    for pos,tile,pred,label in tiles:\n",
    "        if pred[1] < pred[0]:\n",
    "            result_map[\n",
    "                pos[1]:pos[1]+TILE_SIZE,\n",
    "                pos[0]:pos[0]+TILE_SIZE,\n",
    "            ] += float(pred[0]) / 2\n",
    "            # / 2 weil max. zwei tiles die Kachel bestimmen\n",
    "\n",
    "    return result_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e94f8a5f-e728-4614-bba7-86d2c98bd60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moires_of_page( row, results ):\n",
    "    relevant_moires = []\n",
    "    \n",
    "    for r in results:\n",
    "        if re.match(f'^{ row.job }\\..+?\\.{ row.pdf_filename }\\..+', r['img_name']):\n",
    "            dpi = int( re.match(r'.+\\.4c_(\\d+)\\.jpg', r['img_name']).groups()[0] )\n",
    "            out_box = [r['value']['x'],r['value']['y'],r['value']['width'],r['value']['height']]\n",
    "            \n",
    "            # box umrechnen\n",
    "            if target_dpi != dpi:\n",
    "                out_box = [round(val * (target_dpi / dpi)) for val in out_box]\n",
    "    \n",
    "            relevant_moires.append(out_box)\n",
    "\n",
    "    return relevant_moires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4734e48-daa2-46b9-a96f-68ff36ae28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection_over_union( box_a, box_b ):\n",
    "    intersection_box = [\n",
    "        max(box_a[0],box_b[0]),\n",
    "        max(box_a[1],box_b[1]),\n",
    "        min(box_a[0]+box_a[2],box_b[0]+box_b[2]),\n",
    "        min(box_a[1]+box_a[3],box_b[1]+box_b[3]),\n",
    "    ]\n",
    "\n",
    "    if intersection_box[2] - intersection_box[0] < 0 or intersection_box[3] - intersection_box[1] < 0:\n",
    "        return 0\n",
    "\n",
    "    intersection = (intersection_box[2] - intersection_box[0]) * (intersection_box[3] - intersection_box[1])\n",
    "    \n",
    "    union_box = [\n",
    "        min(box_a[0],box_b[0]),\n",
    "        min(box_a[1],box_b[1]),\n",
    "        max(box_a[0]+box_a[2],box_b[0]+box_b[2]),\n",
    "        max(box_a[1]+box_a[3],box_b[1]+box_b[3]),\n",
    "    ]\n",
    "    union = (union_box[2] - union_box[0]) * (union_box[3] - union_box[1])\n",
    "\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ae0d8b1-32cc-4f80-bd37-38f59b06840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_boxes( result_map, threshold=0.5 ):\n",
    "    thresh = np.zeros(result_map.shape).astype('uint8')\n",
    "    thresh[result_map > threshold] = 255\n",
    "\n",
    "    (numLabels, labels, stats, centroids) = cv2.connectedComponentsWithStats(\n",
    "    \tthresh, 4, cv2.CV_32S\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        [b[0],b[1],b[2],b[3]]\n",
    "        for b in stats[1:]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e0698b7-2217-43b8-83c5-6d9b219cbb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_boxes( img, moire_boxes, predicted_boxes ):\n",
    "    colors = {\n",
    "        \"target\" : \"green\",\n",
    "        \"predicted\" : \"red\"\n",
    "    }    \n",
    "\n",
    "    img_out = img.copy().convert('RGB')\n",
    "    draw = ImageDraw.Draw(img_out) \n",
    "\n",
    "    for b in moire_boxes:\n",
    "        draw.rectangle([b[0],b[1],b[0]+b[2],b[1]+b[3]], outline=colors['target'], width=10) \n",
    "\n",
    "    for b in predicted_boxes:\n",
    "        draw.rectangle([b[0],b[1],b[0]+b[2],b[1]+b[3]], outline=colors['predicted'], width=10) \n",
    "\n",
    "    \n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e93a4fa-3c99-46d5-a86a-0d0b0479485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dpi = dotenv['TRAIN_DATA_DPI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e73f35e-3817-44e6-a052-97adef0290af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "0 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "68b64bdc-951b-4a69-a498-65faf13ef8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pages.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "4639bcf9-88e3-4703-a1ba-4ce44fa278a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027df12f839e4edebb8e7d610f6087f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a16bde30924b6ab51104b223044c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516423afd0a4446b804ed1869e67d8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83c35a602ac4f47b75d79bff4264825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca87e08059b446783d7cd0055d640e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82026dd4a06f44cbb54086db0aaa9de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9460cec8e67a4c66aebb19f73c6087cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3fcf52707b488b99562ecb8bb64c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb50540f27344168989b4bcb8646fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36beffacef9b4f01babde5aec12eb82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e57aef6d1b4e38b73d08823e6a0800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(samples.shape[0])):\n",
    "    sample = samples.iloc[i]\n",
    "    \n",
    "    img_path = get_related_filepath(\n",
    "        sample.job,\n",
    "        'ps2400dpi150lpi',\n",
    "        f'{ sample.pdf_filename }.4c_{ dotenv[\"LOFI_DPI\"] }.jpg'\n",
    "    )\n",
    "    img = open_img(img_path)\n",
    "    img = img.resize((\n",
    "        round(img.size[0] * (target_dpi / dotenv[\"LOFI_DPI\"])),\n",
    "        round(img.size[1] * (target_dpi / dotenv[\"LOFI_DPI\"]))\n",
    "    ))\n",
    "    tiles = cut_img_into_tiles( img )\n",
    "    tiles_preclassified = preclassifier( tiles )\n",
    "    predictions = classifier( tiles_preclassified )\n",
    "    \n",
    "    result_map = create_result_map( img.size, predictions )\n",
    "    relevant_moires = get_moires_of_page( sample, results )\n",
    "    \n",
    "    predicted_label = int(result_map[result_map > 0.5].shape[0] > 0)\n",
    "\n",
    "    # blend img erzeugen\n",
    "    TARGET_OUT_HEIGHT = 1000\n",
    "    colors=['red','green','blue','orange']\n",
    "    l_images = get_out_map( img, predictions )\n",
    "    blended = Image.new(mode=\"RGB\", size=(img.size[0] * len(l_images), img.size[1]))\n",
    "\n",
    "    for i in range(len(l_images)):\n",
    "        l_img = ImageEnhance.Brightness(l_images[i]).enhance(0.5)\n",
    "        overlay = Image.new('RGB', l_img.size, color=colors[i])\n",
    "        l_rgb = img.convert('RGB')\n",
    "        l_rgb.paste(\n",
    "            overlay,\n",
    "            (0,0),\n",
    "            mask=l_img\n",
    "        )\n",
    "\n",
    "        blended.paste(\n",
    "            l_rgb,\n",
    "            (img.size[0] * i,0)\n",
    "        )\n",
    "\n",
    "    #blended = blended.resize((\n",
    "    #    round(TARGET_OUT_HEIGHT / blended.size[1] * blended.size[0]),\n",
    "    #    TARGET_OUT_HEIGHT\n",
    "    #))\n",
    "    \n",
    "    blended.save( dotenv['TEMP_PROCESSING_DIR'] / 'blended_page_results' / f\"separated.{ sample.job }.{ sample.pdf_filename }.{ predicted_label }.jpg\", progressive=True )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
