{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcf08001-b838-4623-b3e9-7a09093260e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Type, Any, Callable, Union, List, Dict, Optional, cast\n",
    "from torch import Tensor\n",
    "from collections import OrderedDict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6215f41-8f72-4dcf-a266-2d6f91adb72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch.hub import load_state_dict_from_url\n",
    "except ImportError:\n",
    "    from torch.utils.model_zoo import load_url as load_state_dict_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ae3b99-07fa-4c13-ba1b-b5381c3a1b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0687dd50-4efd-40bd-88b0-a422566089ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import *\n",
    "from torchvision.models.resnet import BasicBlock, Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4fe36c-e50b-434a-aeed-c57f6b23b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a90b6147-8172-4a9c-aa40-766db38feec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as T\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfaba177-c87e-483b-902d-b70436923b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aea6e89-f07a-4998-9e1a-e938d446e481",
   "metadata": {},
   "source": [
    "# Daten laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086c8baf-f2e9-40e6-a5d1-540e3821e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"./dataset/train/\"\n",
    "test_dir = \"./dataset/test/\"\n",
    "train_classa_dir = \"./dataset/train/moire/\"\n",
    "train_classb_dir = \"./dataset/train/no_moire/\"\n",
    "test_classa_dir = \"./dataset/test/moire/\"\n",
    "test_classb_dir = \"./dataset/test/no_moire/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8575996-77b4-4fb3-b030-7010f5544923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transform function\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   #must same as here\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(), # data augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization\n",
    "])\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   #must same as here\n",
    "    transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3774d576-536e-4077-97b9-889a9f0b2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(train_dir, transforms_train)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transforms_test)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=0)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=12, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a81b46a-dd54-4b88-a537-4b176f75b097",
   "metadata": {},
   "source": [
    "# Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba4b41a-02d7-4b8c-9409-f791316a9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntResNet(ResNet):\n",
    "    def __init__(self,output_layer,*args):\n",
    "        self.output_layer = output_layer\n",
    "        super().__init__(*args)\n",
    "        \n",
    "        self._layers = []\n",
    "        for l in list(self._modules.keys()):\n",
    "            self._layers.append(l)\n",
    "            if l == output_layer:\n",
    "                break\n",
    "        self.layers = OrderedDict(zip(self._layers,[getattr(self,l) for l in self._layers]))\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        for l in self._layers:\n",
    "            x = self.layers[l](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13528ba2-5493-44be-bed7-46bb5a7586df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_resnet(\n",
    "    arch: str,\n",
    "    outlayer: str,\n",
    "    block: Type[Union[BasicBlock, Bottleneck]],\n",
    "    layers: List[int],\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any\n",
    "):\n",
    "    '''model_urls = {\n",
    "        'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "        'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "        'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "        'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "        'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "        'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "        'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "        'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "        'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "    }'''\n",
    "    model = IntResNet(outlayer, block, layers, **kwargs)\n",
    "    \n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(ResNet50_Weights.IMAGENET1K_V2.url,progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd32fe58-1196-46a1-9796-497f960397bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        self.resnet = new_resnet('resnet50','layer2',Bottleneck, [3, 4, 6, 3],True,True)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear1 = torch.nn.Linear(401408, 500)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(500, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "tinymodel = TinyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "133d83a5-f5b6-400f-9227-e1836776bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4538a7fa-366b-4b05-baeb-2c4778eaf14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c00653a-e0da-4eb6-ac3a-18c73242ef35",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dfe55cf-fc0a-441e-89e9-3624ea887cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5cb8e2f-183d-41fd-8187-c222ac391ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seeds\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abd237b1-ea5c-4b61-a2cb-102443099f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d8112b0-c1db-4adc-9637-41b824d59ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res net freezen\n",
    "for param in list(model.children())[0].parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b44ef939-6d47-4eba-bc9f-aa37d71f1aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 running\n",
      "[Train #1] Loss: 0.0661 Acc: 58.5938% Time: 14.1584s\n",
      "[Test #1] Loss: 0.0865 Acc: 46.9136% Time: 15.6410s\n",
      "Epoch 1 running\n",
      "[Train #2] Loss: 0.0475 Acc: 74.6094% Time: 25.7054s\n",
      "[Test #2] Loss: 0.0281 Acc: 90.1235% Time: 27.3563s\n",
      "Epoch 2 running\n",
      "[Train #3] Loss: 0.0383 Acc: 79.6875% Time: 37.1541s\n",
      "[Test #3] Loss: 0.0405 Acc: 91.3580% Time: 38.9033s\n",
      "Epoch 3 running\n",
      "[Train #4] Loss: 0.0349 Acc: 82.0312% Time: 49.0883s\n",
      "[Test #4] Loss: 0.0489 Acc: 64.1975% Time: 50.7117s\n",
      "Epoch 4 running\n",
      "[Train #5] Loss: 0.0358 Acc: 81.2500% Time: 60.8906s\n",
      "[Test #5] Loss: 0.0558 Acc: 93.8272% Time: 62.5478s\n",
      "Epoch 5 running\n",
      "[Train #6] Loss: 0.0390 Acc: 80.4688% Time: 73.1314s\n",
      "[Test #6] Loss: 0.0204 Acc: 92.5926% Time: 74.5074s\n",
      "Epoch 6 running\n",
      "[Train #7] Loss: 0.0419 Acc: 78.1250% Time: 83.5300s\n",
      "[Test #7] Loss: 0.0263 Acc: 91.3580% Time: 85.0217s\n",
      "Epoch 7 running\n",
      "[Train #8] Loss: 0.0423 Acc: 79.6875% Time: 94.2561s\n",
      "[Test #8] Loss: 0.0208 Acc: 91.3580% Time: 95.8915s\n",
      "Epoch 8 running\n",
      "[Train #9] Loss: 0.0316 Acc: 82.8125% Time: 106.1813s\n",
      "[Test #9] Loss: 0.0250 Acc: 93.8272% Time: 107.7557s\n",
      "Epoch 9 running\n",
      "[Train #10] Loss: 0.0373 Acc: 78.1250% Time: 117.8715s\n",
      "[Test #10] Loss: 0.0230 Acc: 88.8889% Time: 119.3795s\n",
      "Epoch 10 running\n",
      "[Train #11] Loss: 0.0454 Acc: 74.2188% Time: 131.2951s\n",
      "[Test #11] Loss: 0.0191 Acc: 95.0617% Time: 133.3467s\n",
      "Epoch 11 running\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# forward inputs and get output\u001b[39;00m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# get loss value and update the network weights\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m, in \u001b[0;36mTinyModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(x)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m, in \u001b[0;36mIntResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m, in \u001b[0;36mIntResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers:\n\u001b[0;32m---> 15\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torchvision/models/resnet.py:158\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n\u001b[1;32m    161\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### Train model\n",
    "train_loss=[]\n",
    "train_accuary=[]\n",
    "test_loss=[]\n",
    "test_accuary=[]\n",
    "\n",
    "num_epochs = 30   #(set no of epochs)\n",
    "start_time = time.time() #(for showing time)\n",
    "# Start loop\n",
    "for epoch in range(num_epochs): #(loop for every epoch)\n",
    "    print(\"Epoch {} running\".format(epoch)) #(printing message)\n",
    "    \"\"\" Training Phase \"\"\"\n",
    "    model.train()    #(training model)\n",
    "    running_loss = 0.   #(set loss 0)\n",
    "    running_corrects = 0 \n",
    "    # load a batch data of images\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.reshape((-1,1)).float().to(device)\n",
    "        # forward inputs and get output\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # get loss value and update the network weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(labels.data == outputs.round()).item()\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects / len(train_dataset) * 100.\n",
    "    # Append result\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_accuary.append(epoch_acc)\n",
    "    # Print progress\n",
    "    print('[Train #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch+1, epoch_loss, epoch_acc, time.time() -start_time))\n",
    "    \"\"\" Testing Phase \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.\n",
    "        running_corrects = 0\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.reshape((-1,1)).float().to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            running_corrects += torch.sum(labels.data == outputs.round()).item()\n",
    "        epoch_loss = running_loss / len(test_dataset)\n",
    "        epoch_acc = running_corrects / len(test_dataset) * 100.\n",
    "        # Append result\n",
    "        test_loss.append(epoch_loss)\n",
    "        test_accuary.append(epoch_acc)\n",
    "        # Print progress\n",
    "        print('[Test #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch+1, epoch_loss, epoch_acc, time.time()- start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77497dc-31a8-4d2b-ae1b-b2eccb8d5a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdca238a-5399-48e7-9d12-98bf85e2834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resnet.layers['layer1'][1].conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33661c-6afd-4142-9aba-0bc58385ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    x=np.arange(1,num_epochs+1),\n",
    "    y=train_accuary\n",
    ")\n",
    "\n",
    "# Only thing I figured is - I could do this \n",
    "fig.add_scatter(\n",
    "    x=np.arange(1,num_epochs+1),\n",
    "    y=test_accuary\n",
    ") # Not what is desired - need a line \n",
    "\n",
    "# Show plot \n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
