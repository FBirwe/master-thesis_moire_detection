{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b373706-a659-432f-aaba-a91ce0cfe4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../30_data_tools/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff88dc26-fa26-41e5-9b6c-73dff438135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch\n",
    "from helper import load_dotenv\n",
    "import plotly.express as px\n",
    "import math\n",
    "from random import choices\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eeb15fe-0634-4bdf-b9ab-e40706321c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69571fde-4e46-4c6f-9cdd-87773bac4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_paths = list((dotenv['TILE_DATASET_DIR'] / 'train' / 'moire').glob('./*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169cbc17-9bf7-44a5-b52f-bca4b7feb3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_MAP = torch.zeros((224,224))\n",
    "\n",
    "for y in range(224):\n",
    "    R_MAP[y,:] = torch.hypot(torch.arange(224) - 112, torch.Tensor([y - 112]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d10e7edd-6162-4a77-8ac3-25a4fde58e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fft( input_img ):\n",
    "    ft = np.fft.ifftshift(np.array(input_img))\n",
    "    ft = np.fft.fft2(ft)\n",
    "    ft = np.fft.fftshift(ft)\n",
    "    \n",
    "    return ft\n",
    "\n",
    "def get_fft_torch( input_img ):\n",
    "    img_torch = torchvision.transforms.functional.to_tensor(input_img).squeeze(0) * 255\n",
    "    ft = torch.fft.ifftshift( img_torch )\n",
    "    ft = torch.fft.fft2( ft )\n",
    "    ft = torch.fft.fftshift( ft )\n",
    "\n",
    "    return ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e1ba9f0-0cfd-4dd9-8504-eb70320cda9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_frequencies( fft, inner_limit=None, outer_limit=None ):\n",
    "    center = (fft.shape[1] / 2, fft.shape[0] / 2)\n",
    "    for y in range(fft.shape[0]):\n",
    "        for x in range(fft.shape[1]):\n",
    "            r = math.sqrt( abs(center[0] - x) ** 2 + abs(center[1] - y) ** 2 )\n",
    "            \n",
    "            if outer_limit is not None and r > outer_limit:\n",
    "                fft[y,x] = 1\n",
    "    \n",
    "            if inner_limit is not None and r < inner_limit:\n",
    "                fft[y,x] = 1\n",
    "\n",
    "    return fft\n",
    "\n",
    "\n",
    "def limit_frequencies_torch( fft, inner_limit=None, outer_limit=None ):\n",
    "    if (inner_limit is None) == False:\n",
    "        fft[R_MAP <= inner_limit] = 1\n",
    "\n",
    "    if (outer_limit is None) == False:\n",
    "        fft[R_MAP >= outer_limit] = 1\n",
    "    \n",
    "    return fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc07ad3e-f962-4af6-baf8-42f2a1730adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency_representation( img ):\n",
    "    fft = np.abs( limit_frequencies( get_fft(img), inner_limit=5 ) )\n",
    "    fft = gaussian_filter(fft, sigma=3)\n",
    "\n",
    "    return fft\n",
    "\n",
    "\n",
    "def get_frequency_representation_torch( img ):\n",
    "    fft = torch.abs( limit_frequencies_torch( get_fft_torch(img), inner_limit=5 ) ).reshape((1,224,224))    \n",
    "    fft = torchvision.transforms.functional.gaussian_blur(fft, (11,11), sigma=3)[0,:,:]\n",
    "\n",
    "    return fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ade89b1-6f2c-4139-8c0b-8af7c6258ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_fft( img ):\n",
    "    fft = get_frequency_representation(img.convert('L'))\n",
    "    channel = (fft / fft.max() * 255).astype('uint8')\n",
    "    fft = np.zeros((channel.shape[0],channel.shape[1],3)).astype('uint8')\n",
    "    fft[:,:,0] = channel\n",
    "    fft[:,:,1] = channel\n",
    "    fft[:,:,2] = channel\n",
    "\n",
    "    return fft\n",
    "\n",
    "\n",
    "def img_to_fft_torch( img ):\n",
    "    fft = get_frequency_representation_torch(img.convert('L'))\n",
    "    channel = (fft / fft.max() * 255)\n",
    "    fft = torch.zeros((channel.shape[0],channel.shape[1],3))\n",
    "    fft[:,:,0] = channel\n",
    "    fft[:,:,1] = channel\n",
    "    fft[:,:,2] = channel\n",
    "\n",
    "    return fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffe35487-d4ed-4d60-a464-f3372c18be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15481aa2-9a87-4a5f-bc4e-9269f5874991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7882, 0.7059, 0.7843,  ..., 0.0667, 0.0706, 0.0510],\n",
       "         [0.7020, 0.7843, 0.7333,  ..., 0.0471, 0.0471, 0.0314],\n",
       "         [0.7922, 0.7176, 0.7961,  ..., 0.0510, 0.0549, 0.0471],\n",
       "         ...,\n",
       "         [0.7804, 0.8667, 0.7529,  ..., 0.8471, 0.8471, 0.8627],\n",
       "         [0.7255, 0.7216, 0.7686,  ..., 0.7922, 0.8824, 0.8353],\n",
       "         [0.8000, 0.8510, 0.7529,  ..., 0.9255, 0.8157, 0.8627]],\n",
       "\n",
       "        [[0.7882, 0.7059, 0.7843,  ..., 0.0667, 0.0706, 0.0510],\n",
       "         [0.7020, 0.7843, 0.7333,  ..., 0.0471, 0.0471, 0.0314],\n",
       "         [0.7922, 0.7176, 0.7961,  ..., 0.0510, 0.0549, 0.0471],\n",
       "         ...,\n",
       "         [0.7804, 0.8667, 0.7529,  ..., 0.8471, 0.8471, 0.8627],\n",
       "         [0.7255, 0.7216, 0.7686,  ..., 0.7922, 0.8824, 0.8353],\n",
       "         [0.8000, 0.8510, 0.7529,  ..., 0.9255, 0.8157, 0.8627]],\n",
       "\n",
       "        [[0.7882, 0.7059, 0.7843,  ..., 0.0667, 0.0706, 0.0510],\n",
       "         [0.7020, 0.7843, 0.7333,  ..., 0.0471, 0.0471, 0.0314],\n",
       "         [0.7922, 0.7176, 0.7961,  ..., 0.0510, 0.0549, 0.0471],\n",
       "         ...,\n",
       "         [0.7804, 0.8667, 0.7529,  ..., 0.8471, 0.8471, 0.8627],\n",
       "         [0.7255, 0.7216, 0.7686,  ..., 0.7922, 0.8824, 0.8353],\n",
       "         [0.8000, 0.8510, 0.7529,  ..., 0.9255, 0.8157, 0.8627]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformation(tile_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "569d1e88-4f5d-4d0c-ab86-2e69f3155cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_selection = choices( tile_paths, k=100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b300f140-b793-4f3d-a761-750090ebb1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e9b2e04-6569-46cf-a797-11401f555265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.1221)\n",
      "tensor(-0.1420)\n",
      "tensor(0.4367)\n",
      "tensor(-0.1077)\n",
      "tensor(0.7928)\n",
      "tensor(-0.9919)\n",
      "tensor(-1.1413)\n",
      "tensor(-0.8876)\n",
      "tensor(-1.2091)\n",
      "tensor(-1.6106)\n",
      "tensor(-0.7549)\n",
      "tensor(-1.0580)\n",
      "tensor(0.0373)\n",
      "tensor(-1.2519)\n",
      "tensor(0.2472)\n",
      "tensor(-1.0200)\n",
      "tensor(-0.2944)\n",
      "tensor(-0.9336)\n",
      "tensor(-1.5914)\n",
      "tensor(-0.5373)\n",
      "tensor(-1.1958)\n",
      "tensor(-1.2909)\n",
      "tensor(0.3818)\n",
      "tensor(-0.1230)\n",
      "tensor(0.3658)\n",
      "tensor(0.0854)\n",
      "tensor(0.3932)\n",
      "tensor(-1.2858)\n",
      "tensor(-0.8522)\n",
      "tensor(-0.2268)\n",
      "tensor(-0.9675)\n",
      "tensor(-0.7761)\n",
      "tensor(-0.0938)\n",
      "tensor(-0.3343)\n",
      "tensor(-0.6338)\n",
      "tensor(-0.7156)\n",
      "tensor(-0.3466)\n",
      "tensor(-0.5789)\n",
      "tensor(-0.0419)\n",
      "tensor(-1.1488)\n",
      "tensor(-0.0120)\n",
      "tensor(-0.0454)\n",
      "tensor(-0.5175)\n",
      "tensor(-0.3274)\n",
      "tensor(-0.7052)\n",
      "tensor(-0.5568)\n",
      "tensor(-1.4842)\n",
      "tensor(0.7021)\n",
      "tensor(-2.0768)\n",
      "tensor(0.0678)\n",
      "tensor(-1.0691)\n",
      "tensor(-0.7315)\n",
      "tensor(-0.7802)\n",
      "tensor(-1.6545)\n",
      "tensor(-0.0579)\n",
      "tensor(-0.5366)\n",
      "tensor(-0.5989)\n",
      "tensor(-1.1126)\n",
      "tensor(-0.8457)\n",
      "tensor(-1.6733)\n",
      "tensor(0.0210)\n",
      "tensor(-1.8718)\n",
      "tensor(0.1032)\n",
      "tensor(-1.5385)\n",
      "tensor(-0.8312)\n",
      "tensor(0.1961)\n",
      "tensor(-0.4080)\n",
      "tensor(-1.1640)\n",
      "tensor(0.7585)\n",
      "tensor(-1.0889)\n",
      "tensor(-1.5505)\n",
      "tensor(-1.6245)\n",
      "tensor(-1.2843)\n",
      "tensor(-0.7385)\n",
      "tensor(-0.2173)\n",
      "tensor(-1.0156)\n",
      "tensor(-0.9130)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m torch_fft \u001b[38;5;241m=\u001b[39m img_to_fft_torch( tile_img )    \n\u001b[1;32m      4\u001b[0m numpy_fft \u001b[38;5;241m=\u001b[39m img_to_fft( tile_img )\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[43m(\u001b[49m\u001b[43mtorch_fft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnumpy_fft\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for tile_path in test_selection:\n",
    "    tile_img = Image.open( tile_path )\n",
    "    torch_fft = img_to_fft_torch( tile_img )    \n",
    "    numpy_fft = img_to_fft( tile_img )\n",
    "\n",
    "    print( (torch_fft - numpy_fft).mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1be961-00f8-4853-b74a-cfbb15611eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714abe8e-e7d2-43b4-80d2-cd0678b2151c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa293e8f-90ef-4a04-8f33-9d55d0688f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.abs(torch_fft - numpy_fft).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a15006-3004-46af-aaee-3605dc6c1780",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8569d3b-1b51-4dae-9e3f-9b0141e02c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(tile_img.convert('L'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f6603c-160b-4d50-a12e-9b0b148a0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_torch = to_tensor(tile_img.convert('L')).squeeze(0) * 255\n",
    "fft = torch.fft.ifftshift( img_torch )\n",
    "fft = torch.fft.fft2( fft )\n",
    "fft = torch.fft.fftshift( fft )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe6461e-24fe-4ee5-a55f-a1443289251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft[:5,:5].real - get_fft( tile_img.convert('L') )[:5,:5].real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e5bb9-bdba-40bd-9010-12f956d944be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft[:5,:5].real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f757a86e-5db5-44d1-a50f-ee59e9a3a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fft( tile_img.convert('L') )[:5,:5].real"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
